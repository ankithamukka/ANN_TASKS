{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f99d665-987d-4bd3-90d4-e7371c76f5f9",
   "metadata": {},
   "source": [
    "Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed23a4d7-6b60-4e73-82d7-536ab5b33a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into Training, Validation, and Testing sets.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Define the paths for the input and output folders\n",
    "input_folder = \"/home/ankitha-mukka/Videos/Orchid/\"\n",
    "output_folder_1 = \"/home/ankitha-mukka/Train\"\n",
    "output_folder_2 = \"/home/ankitha-mukka/Valid\"\n",
    "output_folder_3 = \"/home/ankitha-mukka/Test\"\n",
    "\n",
    "# Ensure output directories exist, if not, create them\n",
    "for folder in [output_folder_1, output_folder_2, output_folder_3]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "# Get the list of image files in the input folder\n",
    "image_files = os.listdir(input_folder)\n",
    "\n",
    "# Calculate the number of images for each category\n",
    "total_images = len(image_files)\n",
    "category_1_count = int(total_images * 0.8)\n",
    "category_2_count = int(total_images * 0.1)\n",
    "category_3_count = total_images - category_1_count - category_2_count\n",
    "\n",
    "# Shuffle the image files randomly\n",
    "random.shuffle(image_files)\n",
    "\n",
    "# Initialize counters for each category\n",
    "count_1, count_2, count_3 = 0, 0, 0\n",
    "\n",
    "# Move the images to the respective folders based on the distribution\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(input_folder, image_file)\n",
    "    \n",
    "    if count_1 < category_1_count:\n",
    "        output_path = os.path.join(output_folder_1, image_file)\n",
    "        count_1 += 1\n",
    "    elif count_2 < category_2_count:\n",
    "        output_path = os.path.join(output_folder_2, image_file)\n",
    "        count_2 += 1\n",
    "    else:\n",
    "        output_path = os.path.join(output_folder_3, image_file)\n",
    "        count_3 += 1\n",
    "    \n",
    "    try:\n",
    "        # Move the file to the destination folder\n",
    "        shutil.move(image_path, output_path)\n",
    "        print(f\"Moved {image_file} to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error moving {image_file} to {output_path}: {e}\")\n",
    "\n",
    "print(\"Data split into Training, Validation, and Testing sets.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b471f3ad-36e2-4077-b2ca-abe62a15017e",
   "metadata": {},
   "source": [
    "1.Implement a perceptron from scratch  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab0e1496-9ee1-4142-a9d8-c000c7f37748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Training Accuracy: 1.00\n",
      "Validation Accuracy: 1.00\n",
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2  # Assuming you have OpenCV installed for image processing\n",
    "\n",
    "# Define paths to your training, validation, and testing data\n",
    "output_folder_1 = \"/home/ankitha-mukka/Train\"\n",
    "output_folder_2 = \"/home/ankitha-mukka/Valid\"\n",
    "output_folder_3 = \"/home/ankitha-mukka/Test\"\n",
    "\n",
    "# Initialize perceptron parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1\n",
    "input_shape = (28, 28, 3)  # Assuming images are resized to 28x28 and have 3 channels\n",
    "\n",
    "# Initialize perceptron weights and bias\n",
    "num_inputs = np.prod(input_shape)\n",
    "weights = np.zeros(num_inputs)\n",
    "bias = 0.0\n",
    "\n",
    "# Function to load data from folders\n",
    "X_train = []\n",
    "y_train = []\n",
    "for root, dirs, files in os.walk(output_folder_1):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "            X_train.append(img.flatten())\n",
    "            y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Training the perceptron\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(len(X_train)):\n",
    "        linear_output = np.dot(weights, X_train[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        error = y_train[i] - prediction\n",
    "        weights += learning_rate * error * X_train[i]\n",
    "        bias += learning_rate * error\n",
    "\n",
    "    # Calculate training accuracy after each epoch\n",
    "    correct_train = 0\n",
    "    for i in range(len(X_train)):\n",
    "        linear_output = np.dot(weights, X_train[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        if prediction == y_train[i]:\n",
    "            correct_train += 1\n",
    "    training_accuracy = correct_train / len(X_train)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {training_accuracy:.2f}\")\n",
    "\n",
    "# Function to test accuracy on a dataset\n",
    "def test_accuracy(X, y, weights, bias):\n",
    "    correct = 0\n",
    "    for i in range(len(X)):\n",
    "        linear_output = np.dot(weights, X[i]) + bias\n",
    "        prediction = 1 if linear_output >= 0 else 0\n",
    "        if prediction == y[i]:\n",
    "            correct += 1\n",
    "    return correct / len(X)\n",
    "\n",
    "# Load validation data and test accuracy\n",
    "X_val = []\n",
    "y_val = []\n",
    "for root, dirs, files in os.walk(output_folder_2):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "            X_val.append(img.flatten())\n",
    "            y_val.append(label)\n",
    "\n",
    "X_val = np.array(X_val)\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "validation_accuracy = test_accuracy(X_val, y_val, weights, bias)\n",
    "print(f\"Validation Accuracy: {validation_accuracy:.2f}\")\n",
    "\n",
    "# Load testing data and test accuracy\n",
    "X_test = []\n",
    "y_test = []\n",
    "for root, dirs, files in os.walk(output_folder_3):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "            X_test.append(img.flatten())\n",
    "            y_test.append(label)\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "test_accuracy = test_accuracy(X_test, y_test, weights, bias)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a7c85b-9d55-4235-890d-1c641aa103ec",
   "metadata": {},
   "source": [
    "2 Build and train a simple neural network using a framework like TensorFlow or PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "410f99f2-a8e3-4462-a311-f4239b20a94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 1.0000 - loss: 0.0295 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.5827e-37 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 3/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.0513e-39 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 4/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 8.1767e-39 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 5/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 9.1463e-39 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 6/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.5783e-39 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 7/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 6.1312e-39 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 8/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 3.4366e-38 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 9/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 1.7450e-38 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 10/10\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.8526e-38 - val_accuracy: 1.0000 - val_loss: 0.0000e+00\n",
      "Epoch 1/10, Training Accuracy: 1.00\n",
      "Epoch 2/10, Training Accuracy: 1.00\n",
      "Epoch 3/10, Training Accuracy: 1.00\n",
      "Epoch 4/10, Training Accuracy: 1.00\n",
      "Epoch 5/10, Training Accuracy: 1.00\n",
      "Epoch 6/10, Training Accuracy: 1.00\n",
      "Epoch 7/10, Training Accuracy: 1.00\n",
      "Epoch 8/10, Training Accuracy: 1.00\n",
      "Epoch 9/10, Training Accuracy: 1.00\n",
      "Epoch 10/10, Training Accuracy: 1.00\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0000e+00 \n",
      "Test Accuracy: 1.00\n",
      "Validation Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define paths to your training, validation, and testing data\n",
    "output_folder_1 = \"/home/ankitha-mukka/Train\"\n",
    "output_folder_2 = \"/home/ankitha-mukka/Valid\"\n",
    "output_folder_3 = \"/home/ankitha-mukka/Test\"\n",
    "\n",
    "# Initialize parameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10  # Increase the number of epochs for better training\n",
    "input_shape = (28, 28, 3)\n",
    "\n",
    "# Function to load data from folders\n",
    "def load_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"positive\" in root else 0\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.resize(img, (input_shape[0], input_shape[1]))\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = load_data(output_folder_1)\n",
    "X_val, y_val = load_data(output_folder_2)\n",
    "X_test, y_test = load_data(output_folder_3)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=input_shape),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "# Extract training accuracy from history\n",
    "training_accuracy = history.history['accuracy']\n",
    "validation_accuracy = history.history['val_accuracy']\n",
    "\n",
    "# Print training accuracy for each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Accuracy: {training_accuracy[epoch]:.2f}\")\n",
    "\n",
    "# Evaluate on test data\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Print final validation accuracy\n",
    "print(f\"Validation Accuracy: {validation_accuracy[-1]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722c5395-29b3-4ec0-b390-04b647100cce",
   "metadata": {},
   "source": [
    "3.Create a multi-layer perceptron (MLP) for digit classification (MNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3221c1-acfc-4113-980d-25bd61150ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.43284472165949317\n",
      "Epoch 2, Loss: 0.17986170040481309\n",
      "Epoch 3, Loss: 0.1315260052903612\n",
      "Epoch 4, Loss: 0.10683441584931812\n",
      "Epoch 5, Loss: 0.08848909885357178\n",
      "Epoch 6, Loss: 0.0764847566143052\n",
      "Epoch 7, Loss: 0.06537720433032794\n",
      "Epoch 8, Loss: 0.058769694566001104\n",
      "Epoch 9, Loss: 0.054030158151097185\n",
      "Epoch 10, Loss: 0.046806890753223564\n",
      "Training Accuracy: 98.90833333333333 %\n",
      "Test Accuracy: 97.39 %\n",
      "Validation Accuracy: 97.39 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define transforms for preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Load data from folders\n",
    "train_data = datasets.MNIST('/home/ankitha-mukka/Train', download=True, train=True, transform=transform)\n",
    "validation_data = datasets.MNIST('/home/ankitha-mukka/Valid', download=True, train=False, transform=transform)\n",
    "test_data = datasets.MNIST('/home/ankitha-mukka/Test', download=True, train=False, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Multi-layer perceptron (MLP) model\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(784, 128),  # input layer (28x28) -> hidden layer (128)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),  # hidden layer (128) -> hidden layer (64)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 10)  # hidden layer (64) -> output layer (10)\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/i}')\n",
    "\n",
    "# Evaluate on training set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Training Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Test Accuracy: {100 * correct / total} %')\n",
    "\n",
    "# Evaluate on validation set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in validation_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.view(-1, 784)\n",
    "        outputs = mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Validation Accuracy: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0778894b-fef2-43f6-bdd6-8207ea4c5ed5",
   "metadata": {},
   "source": [
    "4. Experiment with different regularization techniques on a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2291893-0313-45ea-9cc6-50907c8c2091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ankitha-mukka/.local/lib/python3.10/site-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9775 - loss: 7.2333 - val_accuracy: 1.0000 - val_loss: 4.1536\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 3.3879 - val_accuracy: 1.0000 - val_loss: 1.6686\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 1.3827 - val_accuracy: 1.0000 - val_loss: 0.7521\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.6544 - val_accuracy: 1.0000 - val_loss: 0.4223\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.3902 - val_accuracy: 1.0000 - val_loss: 0.2839\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.2658 - val_accuracy: 1.0000 - val_loss: 0.2056\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1986 - val_accuracy: 1.0000 - val_loss: 0.1619\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1589 - val_accuracy: 1.0000 - val_loss: 0.1371\n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1376 - val_accuracy: 1.0000 - val_loss: 0.1189\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1210 - val_accuracy: 1.0000 - val_loss: 0.1066\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1079 - val_accuracy: 1.0000 - val_loss: 0.0942\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0971 - val_accuracy: 1.0000 - val_loss: 0.0856\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0872 - val_accuracy: 1.0000 - val_loss: 0.0776\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0816 - val_accuracy: 1.0000 - val_loss: 0.0719\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0735 - val_accuracy: 1.0000 - val_loss: 0.0662\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0697 - val_accuracy: 1.0000 - val_loss: 0.0619\n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0637 - val_accuracy: 1.0000 - val_loss: 0.0584\n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0600 - val_accuracy: 1.0000 - val_loss: 0.0549\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0563 - val_accuracy: 1.0000 - val_loss: 0.0523\n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0532 - val_accuracy: 1.0000 - val_loss: 0.0489\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0489 \n",
      "\n",
      "Results with L1 Regularization and Dropout:\n",
      "Training Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0489 \n",
      "Validation Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0488 \n",
      "Test Accuracy: 1.0000\n",
      "Epoch 1/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9270 - loss: 0.4726 - val_accuracy: 1.0000 - val_loss: 0.2721\n",
      "Epoch 2/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9989 - loss: 0.2617 - val_accuracy: 1.0000 - val_loss: 0.2300\n",
      "Epoch 3/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.2220 - val_accuracy: 1.0000 - val_loss: 0.1983\n",
      "Epoch 4/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1919 - val_accuracy: 1.0000 - val_loss: 0.1730\n",
      "Epoch 5/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.1677 - val_accuracy: 1.0000 - val_loss: 0.1518\n",
      "Epoch 6/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.1475 - val_accuracy: 1.0000 - val_loss: 0.1343\n",
      "Epoch 7/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.1306 - val_accuracy: 1.0000 - val_loss: 0.1197\n",
      "Epoch 8/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1167 - val_accuracy: 1.0000 - val_loss: 0.1073\n",
      "Epoch 9/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.1048 - val_accuracy: 1.0000 - val_loss: 0.0968\n",
      "Epoch 10/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0947 - val_accuracy: 1.0000 - val_loss: 0.0877\n",
      "Epoch 11/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0858 - val_accuracy: 1.0000 - val_loss: 0.0797\n",
      "Epoch 12/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0783 - val_accuracy: 1.0000 - val_loss: 0.0728\n",
      "Epoch 13/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 1.0000 - loss: 0.0714 - val_accuracy: 1.0000 - val_loss: 0.0666\n",
      "Epoch 14/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0655 - val_accuracy: 1.0000 - val_loss: 0.0611\n",
      "Epoch 15/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0605 - val_accuracy: 1.0000 - val_loss: 0.0562\n",
      "Epoch 16/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0556 - val_accuracy: 1.0000 - val_loss: 0.0517\n",
      "Epoch 17/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0508 - val_accuracy: 1.0000 - val_loss: 0.0476\n",
      "Epoch 18/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 0.0468 - val_accuracy: 1.0000 - val_loss: 0.0440\n",
      "Epoch 19/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0435 - val_accuracy: 1.0000 - val_loss: 0.0407\n",
      "Epoch 20/20\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 1.0000 - loss: 0.0401 - val_accuracy: 1.0000 - val_loss: 0.0375\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0376 \n",
      "\n",
      "Results with L2 Regularization and Dropout:\n",
      "Training Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0375 \n",
      "Validation Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0375 \n",
      "Test Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define paths to your training, validation, and testing data\n",
    "output_folder_1 = \"/home/ankitha-mukka/Train\"\n",
    "output_folder_2 = \"/home/ankitha-mukka/Valid\"\n",
    "output_folder_3 = \"/home/ankitha-mukka/Test\"\n",
    "\n",
    "# Initialize parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "input_shape = (28, 28, 3)  # Assuming images are resized to 28x28 and have 3 channels\n",
    "\n",
    "# Function to load data from folders\n",
    "def load_data(folder_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "                img_path = os.path.join(root, file)\n",
    "                label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "                img = cv2.imread(img_path)\n",
    "                img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Load data\n",
    "X_train, y_train = load_data(output_folder_1)\n",
    "X_val, y_val = load_data(output_folder_2)\n",
    "X_test, y_test = load_data(output_folder_3)\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train = X_train / 255.0\n",
    "X_val = X_val / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Define the neural network model with L1 regularization and dropout\n",
    "model_l1_dropout = Sequential([\n",
    "    Flatten(input_shape=input_shape),\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001)),\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.001)),\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with L1 regularization and dropout\n",
    "model_l1_dropout.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Train the model with L1 regularization and dropout\n",
    "history_l1_dropout = model_l1_dropout.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate on training data with L1 regularization and dropout\n",
    "train_loss_l1_dropout, train_accuracy_l1_dropout = model_l1_dropout.evaluate(X_train, y_train)\n",
    "print(\"\\nResults with L1 Regularization and Dropout:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_l1_dropout:.4f}\")\n",
    "\n",
    "# Evaluate on validation data with L1 regularization and dropout\n",
    "val_loss_l1_dropout, val_accuracy_l1_dropout = model_l1_dropout.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy_l1_dropout:.4f}\")\n",
    "\n",
    "# Evaluate on test data with L1 regularization and dropout\n",
    "test_loss_l1_dropout, test_accuracy_l1_dropout = model_l1_dropout.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy_l1_dropout:.4f}\")\n",
    "\n",
    "# Define the neural network model with L2 regularization and dropout\n",
    "model_l2_dropout = Sequential([\n",
    "    Flatten(input_shape=input_shape),\n",
    "    Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "    Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model with L2 regularization and dropout\n",
    "model_l2_dropout.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                         loss='binary_crossentropy',\n",
    "                         metrics=['accuracy'])\n",
    "\n",
    "# Train the model with L2 regularization and dropout\n",
    "history_l2_dropout = model_l2_dropout.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate on training data with L2 regularization and dropout\n",
    "train_loss_l2_dropout, train_accuracy_l2_dropout = model_l2_dropout.evaluate(X_train, y_train)\n",
    "print(\"\\nResults with L2 Regularization and Dropout:\")\n",
    "print(f\"Training Accuracy: {train_accuracy_l2_dropout:.4f}\")\n",
    "\n",
    "# Evaluate on validation data with L2 regularization and dropout\n",
    "val_loss_l2_dropout, val_accuracy_l2_dropout = model_l2_dropout.evaluate(X_val, y_val)\n",
    "print(f\"Validation Accuracy: {val_accuracy_l2_dropout:.4f}\")\n",
    "\n",
    "# Evaluate on test data with L2 regularization and dropout\n",
    "test_loss_l2_dropout, test_accuracy_l2_dropout = model_l2_dropout.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy_l2_dropout:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3baa23-1566-4293-9b59-e8dca490b51a",
   "metadata": {},
   "source": [
    "5 Compare performance with various optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8167d658-2b8a-44ca-b3dc-6901048b8503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Adam optimizer:\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 1.6041e-07 \n",
      "Training Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 1.9300e-12 \n",
      "Validation Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 2.2357e-12 \n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Training with SGD optimizer:\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 1.0000 - loss: 0.0114 \n",
      "Training Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0079 \n",
      "Validation Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0080 \n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Training with RMSprop optimizer:\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 9.3145e-07\n",
      "Training Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 1.0000 - loss: 6.7474e-10 \n",
      "Validation Accuracy: 1.0000\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 4.9813e-10 \n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Results Summary:\n",
      "Optimizer: Adam\n",
      "Training Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Optimizer: SGD\n",
      "Training Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "\n",
      "Optimizer: RMSprop\n",
      "Training Accuracy: 1.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Test Accuracy: 1.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define paths to your training, validation, and testing data\n",
    "output_folder_1 = \"/home/ankitha-mukka/Train\"\n",
    "output_folder_2 = \"/home/ankitha-mukka/Valid\"\n",
    "output_folder_3 = \"/home/ankitha-mukka/Test\"\n",
    "\n",
    "# Initialize parameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 20\n",
    "input_shape = (28, 28, 3)  # Assuming images are resized to 28x28 and have 3 channels\n",
    "\n",
    "# Load data\n",
    "X_train = []\n",
    "y_train = []\n",
    "for root, _, files in os.walk(output_folder_1):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "            X_train.append(img)\n",
    "            y_train.append(label)\n",
    "X_train = np.array(X_train) / 255.0\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "X_val = []\n",
    "y_val = []\n",
    "for root, _, files in os.walk(output_folder_2):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "            X_val.append(img)\n",
    "            y_val.append(label)\n",
    "X_val = np.array(X_val) / 255.0\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "for root, _, files in os.walk(output_folder_3):\n",
    "    for file in files:\n",
    "        if file.endswith(\".jpg\") or file.endswith(\".png\"):  # Assuming images are in JPG or PNG format\n",
    "            img_path = os.path.join(root, file)\n",
    "            label = 1 if \"positive\" in root else 0  # Example: folder structure decides the label\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (input_shape[0], input_shape[1]))  # Resize image to match input_shape\n",
    "            X_test.append(img)\n",
    "            y_test.append(label)\n",
    "X_test = np.array(X_test) / 255.0\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Define the neural network model\n",
    "models = {\n",
    "    'Adam': Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ]),\n",
    "    'SGD': Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ]),\n",
    "    'RMSprop': Sequential([\n",
    "        Flatten(input_shape=input_shape),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),  # Dropout layer to prevent overfitting\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train and evaluate models with different optimizers\n",
    "results = {}\n",
    "\n",
    "for optimizer_name, model in models.items():\n",
    "    print(f\"\\nTraining with {optimizer_name} optimizer:\")\n",
    "    \n",
    "    if optimizer_name == 'Adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'RMSprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    \n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=num_epochs, validation_data=(X_val, y_val), verbose=0)\n",
    "    \n",
    "    # Evaluate on training data\n",
    "    train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate on validation data\n",
    "    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    results[optimizer_name] = {\n",
    "        'train_accuracy': train_accuracy,\n",
    "        'val_accuracy': val_accuracy,\n",
    "        'test_accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "# Print results summary\n",
    "print(\"\\nResults Summary:\")\n",
    "for optimizer_name, result in results.items():\n",
    "    print(f\"Optimizer: {optimizer_name}\")\n",
    "    print(f\"Training Accuracy: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"Validation Accuracy: {result['val_accuracy']:.4f}\")\n",
    "    print(f\"Test Accuracy: {result['test_accuracy']:.4f}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99f915-5179-4e28-a497-7d0b04879de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bbca1-7e43-45d9-8191-d260e78ea47e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696cd96f-894b-48ee-866a-306d906fda67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d246ce-6034-4552-95d3-d359c9fecb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
